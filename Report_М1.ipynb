{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9fceec",
   "metadata": {},
   "source": [
    "# Модуль А.  Парсинг и предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e9b69",
   "metadata": {},
   "source": [
    "### 1.1 Парсинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d89e62",
   "metadata": {},
   "source": [
    "*Импортируем необходимы библиотеки для дальнейших действий:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a36d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import notebook\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import wordnet \n",
    "from nltk import pos_tag  \n",
    "from nltk import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics import pairwise_distances \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294078d",
   "metadata": {},
   "source": [
    "*Для начало импортируем данные из представленной папки. Они изначально имеют расширение json, так как dataframe лучше рабоет с расширение csv, передем эти файлы в другую папку с данным расширением.Так, как файлов большое количество, импорт будет сделан через корневую дерикторию.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a29a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules = glob.glob('data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7226a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=[]\n",
    "# for i in modules:\n",
    "#     data=pd.read_csv(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4ae47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules = glob.glob('data\\*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca2e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62492bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json=[]\n",
    "# for i in modules:\n",
    "#     data_json=pd.read_json(i)\n",
    "#     data_json.to_csv(f'meeting/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbeef077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json=pd.read_json('sample1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e5eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('meeting/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16b2e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json = pd.read_csv('data\\sample1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b7c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json.to_csv('courses_74.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1e76a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c85ca081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_csv = pd.read_csv('courses_74.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a66ece",
   "metadata": {},
   "source": [
    "*Теперь необходимо собрать данные из сайта habr.com, в котором хранятся статьи связанные с информационными технологиями, бизнесом и интернетом для того, что бы данные увеличились в размере, тогда прогнозирование будет сделано более качественно.А также поместить их в один dataframe, для того, что бы в дальнейшем было удобнее работать.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd0ecff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_str = '{\"Courses\":{\"r1\":\"Spark\"},\"Fee\":{\"r1\":\"25000\"},\"Duration\":{\"r1\":\"50 Days\"}}'\n",
    "# df = pd.read_json(json_str)\n",
    "# df.to_csv('courses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a69b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"courses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1452b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_urls_list = []\n",
    "date=[]\n",
    "for i in range(1, 51):\n",
    "    url = f\"https://habr.com/ru/all/page{i}\"\n",
    "    rec=requests.get(url)\n",
    "    result=rec.content\n",
    "    soup = BeautifulSoup(result, 'lxml')\n",
    "    for tag_3 in soup.find_all(\"span\", {'class':'tm-article-datetime-published'}):\n",
    "        datetime = tag_3.find('time')\n",
    "        film_list_3 = datetime.get('datetime')\n",
    "        date.append(film_list_3)\n",
    "    for tag in soup.find_all(\"a\", {'class':'tm-title__link'}):\n",
    "        area_info = tag.get(\"href\")\n",
    "        areas_urls_list.append(\"https://habr.com\"+area_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f8e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_urls_list_2 = []\n",
    "rating = []\n",
    "text_states=[]\n",
    "for item in areas_urls_list:\n",
    "    url_2 = item\n",
    "    rec_2=requests.get(url_2)\n",
    "    result_2=rec_2.content\n",
    "    soup_2 = BeautifulSoup(result_2, 'lxml')\n",
    "    for tag_4 in soup_2.find_all(\"span\", {'class':'tm-votes-lever__score-counter tm-votes-lever__score-counter tm-votes-lever__score-counter_rating'}):\n",
    "        rating.append(tag_4)\n",
    "   # for tag_6 in soup_2.find_all(\"div\", {'class':'tm-article-presenter'}):\n",
    "        #dey_2 = tag_6.find('a')\n",
    "       # url_3 = dey_2.get(\"title\")\n",
    "       # name.append(url_3)\n",
    "    for tag_5 in soup_2.find_all(\"div\", {'class':'tm-article-presenter'}):\n",
    "        url_5=tag_5.find(\"a\")\n",
    "        url_6 = url_5.get(\"href\")\n",
    "        areas_urls_list_2.append(\"https://habr.com\"+url_6)\n",
    "    for tag_7 in soup_2.find_all(\"div\", {'class':'article-formatted-body article-formatted-body article-formatted-body_version-2'}):\n",
    "        url_7=tag_7.find(\"p\")\n",
    "        text_states.append(url_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c9f0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "href_add=[]\n",
    "for i in range(1, 18):\n",
    "    url_4 = f\"https://habr.com/ru/companies/page{i}\"\n",
    "    rec_4=requests.get(url_4)\n",
    "    result_4=rec_4.content\n",
    "    soup_4 = BeautifulSoup(result_4, 'lxml')\n",
    "    for tag_11 in soup_4.find_all(\"div\", {'class':'tm-company-snippet__info'}):\n",
    "        tag_12=tag_11.find_all(\"a\", {'class':'tm-company-snippet__title'})\n",
    "        name.append(tag_12)\n",
    "    for tag_20 in soup_4.find_all(\"a\", {'class':'tm-company-snippet__title'}):\n",
    "        href = tag_20.get(\"href\")\n",
    "        href_add.append(\"https://habr.com\"+href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "description=[]\n",
    "activity=[]\n",
    "for item in href_add:\n",
    "    url_8 = item\n",
    "    rec_8=requests.get(url_8)\n",
    "    result_8=rec_8.content\n",
    "    soup_8 = BeautifulSoup(result_8, 'lxml')\n",
    "    for tag_4 in soup_8.find_all(\"div\", {'class':'tm-company-profile__categories'}):\n",
    "        tag_5=tag_4.find(\"span\")\n",
    "        tag_15=tag_5.find(\"a\")\n",
    "        activity.append(tag_15)\n",
    "    for tag_7 in soup_8.find_all(\"span\", {'class':'tm-company-profile__content'}):\n",
    "        description.append(tag_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10838ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date=pd.DataFrame(date)\n",
    "rating=pd.DataFrame(rating)\n",
    "activity=pd.DataFrame(activity)\n",
    "description=pd.DataFrame(description)\n",
    "name=pd.DataFrame(name)\n",
    "text_states=pd.DataFrame(text_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cf4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.concat([date, rating, activity, description, name, text_states], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b46338",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['date', 'rating', 'activity', 'description','name', 'text_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b49955",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170787e",
   "metadata": {},
   "source": [
    "*Сохраняем наши данные для дальнейшего использования в других модулях*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3188e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63c4ff",
   "metadata": {},
   "source": [
    "### 1.3 Предварительная обработка текстовых данных  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd2555",
   "metadata": {},
   "source": [
    "*В этом разделе необходимо обработать данные, для того, чтобы в дальнейшем обучение модели происходило более качественно, и в дальнейшем можно было разработать API.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab644512",
   "metadata": {},
   "source": [
    "*С помощью функции $info()$ вывожу информацию о данных*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7954bf",
   "metadata": {},
   "source": [
    "*С помощью функции $describe()$ смотрю статистическую таблицу*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0695b65",
   "metadata": {},
   "source": [
    "*Также необхожимо проанализировать, есть ли в данных пропущенные значения.Для этого волспользуюсь функцией **isnull().sum()**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4153d",
   "metadata": {},
   "source": [
    "*Теперь можно сделать вывод, что большинство значений категориальные, а значит метод обработки данных должен включать обработку текста(токенизацию, лемматизацию, обработку пропущеннных значений, выделение значимых частей речи, а также удаление стоп-слов, пунктуации, спецсимволов). По статистической таблицы видно, что не выбросов, не полхой дисперсии в данных нет, поэтому эти методы обработки применяться не будут.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0baf55",
   "metadata": {},
   "source": [
    "*В начале обработаем все значения, которые пропущенны.Так как данных и так не много, воспользуемся стандартными методом.Для категориальных признаков-заменой на моду числа, для количественных-на медианное значение.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = data.select_dtypes(include=[np.number])\n",
    "numeric_cols = df_numeric.columns.values\n",
    "\n",
    "for col in numeric_cols:\n",
    "    missing = data[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0: \n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        data['{}_ismissing'.format(col)] = missing\n",
    "        med = data[col].median()\n",
    "        data[col] = data[col].fillna(med)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55cde0",
   "metadata": {},
   "source": [
    "*Дальше избавимся от ненужных символов в данных и , а также переведем все значения в нижний регистр, для того, что бы потом было удобнее делать токенизацию и , лемматизацию.Также этот метод обработки сделает качественнным дальнейший прогноз.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text=re.sub('<[^>]*>', ' ', text)\n",
    "    emoticons=re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text=re.sub('[\\W]+', ' ', text.lower())+ ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    data[col]=data[col].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d50b1",
   "metadata": {},
   "source": [
    "*Теперь обработаем весь текст в данных методами nltk.Для начало необходимо разделить текст на токены. Дальше исключю стоп-слова из исходного текста, обработаем стриминг текста(удаление окончания слов)-Русский язык обладает богатой морфологической структурой. Слово хороший и хорошая имеют тот же смысл, но разную форму, например, хорошая мебель и хороший стул. Поэтому для машинного обучения (Machine Learning) лучше привести их к одной форме для уменьшения размерности, приведем к начальному значению.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb3554",
   "metadata": {},
   "source": [
    "**Токенизация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffdcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a694f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['activity'] + ' ' + data['description'] + ' ' + data['name'] + ' ' + data['text_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd206a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized'] = data.apply(lambda x: tokenize(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b7224",
   "metadata": {},
   "source": [
    "**Исключение стоп-слова**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "data['stop_words'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22693feb",
   "metadata": {},
   "source": [
    "**Стриминг**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ff2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "data['stemmer'] = data['text'].str.split()\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1b683",
   "metadata": {},
   "source": [
    "**Лемматизация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "data['text_lemmatized'] = data.text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53658f0",
   "metadata": {},
   "source": [
    "### 1.2 Формирование структуры набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde7ca0",
   "metadata": {},
   "source": [
    "*Для выбора признаков, которые следует удалить из данных, я воспользуюсь алгоритмом SBS. Он основан на последовательном удалении признаков из полнопризнакового подмножества.Я взяла конкретно этот алгоритм, потому-что считаю что он один из немногих может обучить текстовые данные хорошо,а также хорошо может работать с данными маленького размера, так как сделан на основе логистической регрессии.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def16e76",
   "metadata": {},
   "source": [
    "*Для успешного использования алгоритма, необходимо закодировать категориальные значения.Это позволит улучшить точность выбоора признака.Также необходимо разделить выборку на тестовую и тренировочную.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da9853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e0ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7645618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a411d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression(penalty='l1', C=0.1)\n",
    "# lr.fit(X_train_std, y_train)\n",
    "# print('Training accuracy:', lr.score(X_train_std, y_train))\n",
    "# print('Test accuracy:', lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baca43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import clone\n",
    "# from itertools import combinations\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# if Version(sklearn_version) < '0.18':\n",
    "#     from sklearn.cross_validation import train_test_split\n",
    "# else:\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# class SBS():\n",
    "#     def __init__(self, estimator, k_features, scoring=accuracy_score,\n",
    "#                  test_size=0.25, random_state=1):\n",
    "#         self.scoring = scoring\n",
    "#         self.estimator = clone(estimator)\n",
    "#         self.k_features = k_features\n",
    "#         self.test_size = test_size\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "        \n",
    "#         X_train, X_test, y_train, y_test = \\\n",
    "#             train_test_split(X, y, test_size=self.test_size,\n",
    "#                              random_state=self.random_state)\n",
    "\n",
    "#         dim = X_train.shape[1]\n",
    "#         self.indices_ = tuple(range(dim))\n",
    "#         self.subsets_ = [self.indices_]\n",
    "#         score = self._calc_score(X_train, y_train, \n",
    "#                                  X_test, y_test, self.indices_)\n",
    "#         self.scores_ = [score]\n",
    "\n",
    "#         while dim > self.k_features:\n",
    "#             scores = []\n",
    "#             subsets = []\n",
    "\n",
    "#             for p in combinations(self.indices_, r=dim - 1):\n",
    "#                 score = self._calc_score(X_train, y_train, \n",
    "#                                          X_test, y_test, p)\n",
    "#                 scores.append(score)\n",
    "#                 subsets.append(p)\n",
    "\n",
    "#             best = np.argmax(scores)\n",
    "#             self.indices_ = subsets[best]\n",
    "#             self.subsets_.append(self.indices_)\n",
    "#             dim -= 1\n",
    "\n",
    "#             self.scores_.append(scores[best])\n",
    "#         self.k_score_ = self.scores_[-1]\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[:, self.indices_]\n",
    "\n",
    "#     def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "#         self.estimator.fit(X_train[:, indices], y_train)\n",
    "#         y_pred = self.estimator.predict(X_test[:, indices])\n",
    "#         score = self.scoring(y_test, y_pred)\n",
    "#         return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79987e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab8281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f723859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372889c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ad219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d98695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876641f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30750bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e19ed862",
   "metadata": {},
   "source": [
    "### 1.4 Подготовка отчета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a86750",
   "metadata": {},
   "source": [
    "*В данном модули были собраны данные с интернет-ресурса **habr.com** и с представленного файл.Произведено предварительное следование данных для того, что-бы понять, какой конкретно метод обработки использовать.Произведена предобработка данных для дальнейшего обучения модели и разработки api.Также были отобраны признаки, которые больше всего зависят от целевой переменной, а также предварительная обрабоотка текста, что бы разделение на кластеры было более качественно сделано.По итогу файл с предобработанными данными был загружен, для дальнейшего использования.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
