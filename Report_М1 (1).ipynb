{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9fceec",
   "metadata": {},
   "source": [
    "# Модуль А.  Парсинг и предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e9b69",
   "metadata": {},
   "source": [
    "### 1.1 Парсинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d89e62",
   "metadata": {},
   "source": [
    "*Импортируем необходимы библиотеки для дальнейших действий:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a36d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import notebook\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import wordnet \n",
    "from nltk import pos_tag  \n",
    "from nltk import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics import pairwise_distances \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e44082",
   "metadata": {},
   "source": [
    "*Для начало импортируем данные из представленной папки. Они изначально имеют расширение json, так как dataframe лучше рабоет с расширение csv, передем эти файлы в другую папку с данным расширением.Так, как файлов большое количество, импорт будет сделан через корневую дерикторию.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad72fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules = glob.glob('data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d13b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=[]\n",
    "# for i in modules:\n",
    "#     data=pd.read_csv(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c8cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules = glob.glob('data\\*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa7f01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056e8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json=[]\n",
    "# for i in modules:\n",
    "#     data_json=pd.read_json(i)\n",
    "#     data_json.to_csv(f'meeting/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f0f4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json=pd.read_json('sample1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b9ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('meeting/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "689be68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json = pd.read_csv('data\\sample1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c155afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_json.to_csv('courses_74.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c73a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70eabc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_csv = pd.read_csv('courses_74.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a66ece",
   "metadata": {},
   "source": [
    "*Теперь необходимо собрать данные из сайта habr.com, в котором хранятся статьи связанные с информационными технологиями, бизнесом и интернетом для того, что бы данные увеличились в размере, тогда прогнозирование будет сделано более качественно.А также поместить их в один dataframe, для того, что бы в дальнейшем было удобнее работать.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf75102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_str = '{\"Courses\":{\"r1\":\"Spark\"},\"Fee\":{\"r1\":\"25000\"},\"Duration\":{\"r1\":\"50 Days\"}}'\n",
    "# df = pd.read_json(json_str)\n",
    "# df.to_csv('courses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de747d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"courses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1452b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_urls_list = []\n",
    "date=[]\n",
    "for i in range(1, 51):\n",
    "    url = f\"https://habr.com/ru/all/page{i}\"\n",
    "    rec=requests.get(url)\n",
    "    result=rec.content\n",
    "    soup = BeautifulSoup(result, 'lxml')\n",
    "    for tag_3 in soup.find_all(\"span\", {'class':'tm-article-datetime-published'}):\n",
    "        datetime = tag_3.find('time')\n",
    "        film_list_3 = datetime.get('datetime')\n",
    "        date.append(film_list_3)\n",
    "    for tag in soup.find_all(\"a\", {'class':'tm-title__link'}):\n",
    "        area_info = tag.get(\"href\")\n",
    "        areas_urls_list.append(\"https://habr.com\"+area_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f8e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_urls_list_2 = []\n",
    "rating = []\n",
    "text_states=[]\n",
    "for item in areas_urls_list:\n",
    "    url_2 = item\n",
    "    rec_2=requests.get(url_2)\n",
    "    result_2=rec_2.content\n",
    "    soup_2 = BeautifulSoup(result_2, 'lxml')\n",
    "    for tag_4 in soup_2.find_all(\"span\", {'class':'tm-votes-lever__score-counter tm-votes-lever__score-counter tm-votes-lever__score-counter_rating'}):\n",
    "        rating.append(tag_4)\n",
    "   # for tag_6 in soup_2.find_all(\"div\", {'class':'tm-article-presenter'}):\n",
    "        #dey_2 = tag_6.find('a')\n",
    "       # url_3 = dey_2.get(\"title\")\n",
    "       # name.append(url_3)\n",
    "    for tag_5 in soup_2.find_all(\"div\", {'class':'tm-article-presenter'}):\n",
    "        url_5=tag_5.find(\"a\")\n",
    "        url_6 = url_5.get(\"href\")\n",
    "        areas_urls_list_2.append(\"https://habr.com\"+url_6)\n",
    "    for tag_7 in soup_2.find_all(\"div\", {'class':'article-formatted-body article-formatted-body article-formatted-body_version-2'}):\n",
    "        url_7=tag_7.find(\"p\")\n",
    "        text_states.append(url_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c9f0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "href_add=[]\n",
    "for i in range(1, 18):\n",
    "    url_4 = f\"https://habr.com/ru/companies/page{i}\"\n",
    "    rec_4=requests.get(url_4)\n",
    "    result_4=rec_4.content\n",
    "    soup_4 = BeautifulSoup(result_4, 'lxml')\n",
    "    for tag_11 in soup_4.find_all(\"div\", {'class':'tm-company-snippet__info'}):\n",
    "        tag_12=tag_11.find_all(\"a\", {'class':'tm-company-snippet__title'})\n",
    "        name.append(tag_12)\n",
    "    for tag_20 in soup_4.find_all(\"a\", {'class':'tm-company-snippet__title'}):\n",
    "        href = tag_20.get(\"href\")\n",
    "        href_add.append(\"https://habr.com\"+href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5715469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "description=[]\n",
    "activity=[]\n",
    "for item in href_add:\n",
    "    url_8 = item\n",
    "    rec_8=requests.get(url_8)\n",
    "    result_8=rec_8.content\n",
    "    soup_8 = BeautifulSoup(result_8, 'lxml')\n",
    "    for tag_4 in soup_8.find_all(\"div\", {'class':'tm-company-profile__categories'}):\n",
    "        tag_5=tag_4.find(\"span\")\n",
    "        tag_15=tag_5.find(\"a\")\n",
    "        activity.append(tag_15)\n",
    "    for tag_7 in soup_8.find_all(\"span\", {'class':'tm-company-profile__content'}):\n",
    "        description.append(tag_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10838ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blend\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:576: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "date=pd.DataFrame(date)\n",
    "rating=pd.DataFrame(rating)\n",
    "activity=pd.DataFrame(activity)\n",
    "description=pd.DataFrame(description)\n",
    "name=pd.DataFrame(name)\n",
    "text_states=pd.DataFrame(text_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c2cf4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.concat([date, rating, activity, description, name, text_states], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7b46338",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['date', 'rating', 'activity', 'description','name', 'text_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "759f4e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>activity</th>\n",
       "      <th>description</th>\n",
       "      <th>name</th>\n",
       "      <th>text_states</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-27T23:40:01.000Z</td>\n",
       "      <td>\\n        45.8\\n</td>\n",
       "      <td>\\n              Связь и телекоммуникации\\n    ...</td>\n",
       "      <td>[Международный облачный провайдер , [RUVDS],  ...</td>\n",
       "      <td>[RUVDS.com]</td>\n",
       "      <td>[[В предыдущей статье],  я рассказал про проиг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-27T22:02:01.000Z</td>\n",
       "      <td>\\n        0\\n</td>\n",
       "      <td>\\n              Аппаратное обеспечение\\n      ...</td>\n",
       "      <td>[[Selectel],  — ведущий в России провайдер обл...</td>\n",
       "      <td>[Selectel]</td>\n",
       "      <td>[Захотев однажды научиться разрабатывать ботов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-27T21:23:59.000Z</td>\n",
       "      <td>\\n        7\\n</td>\n",
       "      <td>\\n              Веб-разработка\\n</td>\n",
       "      <td>[[ Timeweb Cloud ],  — облачный сервис, сочета...</td>\n",
       "      <td>[Timeweb Cloud]</td>\n",
       "      <td>[Дисплейные методы, как и метод , [гибридóм,],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-27T20:22:08.000Z</td>\n",
       "      <td>\\n        9\\n</td>\n",
       "      <td>\\n              Мобильные технологии\\n        ...</td>\n",
       "      <td>[Строим сервисы, используя силу социальных сет...</td>\n",
       "      <td>[VK]</td>\n",
       "      <td>[Горячая тема! Я раньше очень много и тесно ра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-27T20:10:38.000Z</td>\n",
       "      <td>\\n        0\\n</td>\n",
       "      <td>\\n              Консалтинг и поддержка\\n      ...</td>\n",
       "      <td>[OTUS – сообщество профессионалов, которые пом...</td>\n",
       "      <td>[OTUS]</td>\n",
       "      <td>[В современном мире интернет стал неотъемлемой...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>2023-04-11T08:05:11.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>2023-04-11T07:58:53.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2023-04-11T07:38:32.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2023-04-11T07:25:29.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2023-04-11T07:13:45.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date                  rating  \\\n",
       "0    2023-04-27T23:40:01.000Z  \\n        45.8\\n         \n",
       "1    2023-04-27T22:02:01.000Z     \\n        0\\n         \n",
       "2    2023-04-27T21:23:59.000Z     \\n        7\\n         \n",
       "3    2023-04-27T20:22:08.000Z     \\n        9\\n         \n",
       "4    2023-04-27T20:10:38.000Z     \\n        0\\n         \n",
       "..                        ...                     ...   \n",
       "993  2023-04-11T08:05:11.000Z                     NaN   \n",
       "994  2023-04-11T07:58:53.000Z                     NaN   \n",
       "995  2023-04-11T07:38:32.000Z                     NaN   \n",
       "996  2023-04-11T07:25:29.000Z                     NaN   \n",
       "997  2023-04-11T07:13:45.000Z                     NaN   \n",
       "\n",
       "                                              activity  \\\n",
       "0    \\n              Связь и телекоммуникации\\n    ...   \n",
       "1    \\n              Аппаратное обеспечение\\n      ...   \n",
       "2         \\n              Веб-разработка\\n               \n",
       "3    \\n              Мобильные технологии\\n        ...   \n",
       "4    \\n              Консалтинг и поддержка\\n      ...   \n",
       "..                                                 ...   \n",
       "993                                                NaN   \n",
       "994                                                NaN   \n",
       "995                                                NaN   \n",
       "996                                                NaN   \n",
       "997                                                NaN   \n",
       "\n",
       "                                           description             name  \\\n",
       "0    [Международный облачный провайдер , [RUVDS],  ...      [RUVDS.com]   \n",
       "1    [[Selectel],  — ведущий в России провайдер обл...       [Selectel]   \n",
       "2    [[ Timeweb Cloud ],  — облачный сервис, сочета...  [Timeweb Cloud]   \n",
       "3    [Строим сервисы, используя силу социальных сет...             [VK]   \n",
       "4    [OTUS – сообщество профессионалов, которые пом...           [OTUS]   \n",
       "..                                                 ...              ...   \n",
       "993                                                NaN              NaN   \n",
       "994                                                NaN              NaN   \n",
       "995                                                NaN              NaN   \n",
       "996                                                NaN              NaN   \n",
       "997                                                NaN              NaN   \n",
       "\n",
       "                                           text_states  \n",
       "0    [[В предыдущей статье],  я рассказал про проиг...  \n",
       "1    [Захотев однажды научиться разрабатывать ботов...  \n",
       "2    [Дисплейные методы, как и метод , [гибридóм,],...  \n",
       "3    [Горячая тема! Я раньше очень много и тесно ра...  \n",
       "4    [В современном мире интернет стал неотъемлемой...  \n",
       "..                                                 ...  \n",
       "993                                                NaN  \n",
       "994                                                NaN  \n",
       "995                                                NaN  \n",
       "996                                                NaN  \n",
       "997                                                NaN  \n",
       "\n",
       "[998 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373bcb1",
   "metadata": {},
   "source": [
    "*Сохраняем наши данные для дальнейшего использования в других модулях*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89e7f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63c4ff",
   "metadata": {},
   "source": [
    "### 1.3 Предварительная обработка текстовых данных  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2967a",
   "metadata": {},
   "source": [
    "*В этом разделе необходимо обработать данные, для того, чтобы в дальнейшем обучение модели происходило более качественно, и в дальнейшем можно было разработать API.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016f8e0",
   "metadata": {},
   "source": [
    "*С помощью функции $info()$ вывожу информацию о данных*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34b43586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 998 entries, 0 to 997\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   date         998 non-null    object\n",
      " 1   rating       992 non-null    object\n",
      " 2   activity     322 non-null    object\n",
      " 3   description  322 non-null    object\n",
      " 4   name         322 non-null    object\n",
      " 5   text_states  825 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 342.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6128f",
   "metadata": {},
   "source": [
    "*С помощью функции $describe()$ смотрю статистическую таблицу*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13805d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>998</td>\n",
       "      <td>996</td>\n",
       "      <td>2023-04-21T07:56:23.000Z</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>992</td>\n",
       "      <td>291</td>\n",
       "      <td>\\n        6\\n</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity</th>\n",
       "      <td>322</td>\n",
       "      <td>18</td>\n",
       "      <td>\\n              Программное обеспечение\\n     ...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>322</td>\n",
       "      <td>311</td>\n",
       "      <td>[]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>322</td>\n",
       "      <td>322</td>\n",
       "      <td>[RUVDS.com]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_states</th>\n",
       "      <td>825</td>\n",
       "      <td>808</td>\n",
       "      <td>[Привет, Хабр!]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count unique                                                top  \\\n",
       "date          998    996                           2023-04-21T07:56:23.000Z   \n",
       "rating        992    291                                \\n        6\\n         \n",
       "activity      322     18  \\n              Программное обеспечение\\n     ...   \n",
       "description   322    311                                                 []   \n",
       "name          322    322                                        [RUVDS.com]   \n",
       "text_states   825    808                                    [Привет, Хабр!]   \n",
       "\n",
       "            freq  \n",
       "date           2  \n",
       "rating        37  \n",
       "activity     132  \n",
       "description   12  \n",
       "name           1  \n",
       "text_states    7  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda73e3e",
   "metadata": {},
   "source": [
    "*Также необхожимо проанализировать, есть ли в данных пропущенные значения.Для этого волспользуюсь функцией **isnull().sum()**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8802037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date           0\n",
       "rating         0\n",
       "activity       0\n",
       "description    0\n",
       "name           0\n",
       "text_states    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c921b",
   "metadata": {},
   "source": [
    "*Теперь можно сделать вывод, что большинство значений категориальные, а значит метод обработки данных должен включать обработку текста(токенизацию, лемматизацию, обработку пропущеннных значений, выделение значимых частей речи, а также удаление стоп-слов, пунктуации, спецсимволов). По статистической таблицы видно, что не выбросов, не полхой дисперсии в данных нет, поэтому эти методы обработки применяться не будут.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f6d9b",
   "metadata": {},
   "source": [
    "*В начале обработаем все значения, которые пропущенны.Так как данных и так не много, воспользуемся стандартными методом.Для категориальных признаков-заменой на моду числа, для количественных-на медианное значение.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13ddb05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_numeric = data.select_dtypes(exclude=[np.number])\n",
    "non_numeric_cols = df_non_numeric.columns.values\n",
    "\n",
    "for col in non_numeric_cols:\n",
    "    missing = data[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        data['{}_ismissing'.format(col)] = missing\n",
    "        \n",
    "        top = data[col].describe()['top']\n",
    "        data[col] = data[col].fillna(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0379acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = data.select_dtypes(include=[np.number])\n",
    "numeric_cols = df_numeric.columns.values\n",
    "\n",
    "for col in numeric_cols:\n",
    "    missing = data[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0: \n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        data['{}_ismissing'.format(col)] = missing\n",
    "        med = data[col].median()\n",
    "        data[col] = data[col].fillna(med)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11043c2",
   "metadata": {},
   "source": [
    "*Дальше избавимся от ненужных символов в данных и , а также переведем все значения в нижний регистр, для того, что бы потом было удобнее делать токенизацию и , лемматизацию.Также этот метод обработки сделает качественнным дальнейший прогноз.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c11d1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text=re.sub('<[^>]*>', ' ', text)\n",
    "    emoticons=re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text=re.sub('[\\W]+', ' ', text.lower())+ ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77948aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16b17fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    data[col]=data[col].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f3f17",
   "metadata": {},
   "source": [
    "*Теперь обработаем весь текст в данных методами nltk.Для начало необходимо разделить текст на токены. Дальше исключю стоп-слова из исходного текста, обработаем стриминг текста(удаление окончания слов)-Русский язык обладает богатой морфологической структурой. Слово хороший и хорошая имеют тот же смысл, но разную форму, например, хорошая мебель и хороший стул. Поэтому для машинного обучения (Machine Learning) лучше привести их к одной форме для уменьшения размерности, приведем к начальному значению.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1c90b",
   "metadata": {},
   "source": [
    "**Токенизация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0b51ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\blend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab818369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5533b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['name'] = data.apply(lambda x: tokenize(x['name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6c9d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description'] = data.apply(lambda x: tokenize(x['description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08ef8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['activity'] = data.apply(lambda x: tokenize(x['activity']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1f0a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_states'] = data.apply(lambda x: tokenize(x['text_states']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b11b3",
   "metadata": {},
   "source": [
    "**Исключение стоп-слова**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb8e800f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrussian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrussian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (stop_words)]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "data['description'] = data['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633e03d",
   "metadata": {},
   "source": [
    "**Стриминг**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "data['stemmer'] = data['text'].str.split()\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4214aef",
   "metadata": {},
   "source": [
    "**Лемматизация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "data['text_lemmatized'] = data.text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed420848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09106e20",
   "metadata": {},
   "source": [
    "### 1.2 Формирование структуры набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90456eba",
   "metadata": {},
   "source": [
    "*Для выбора признаков, которые следует удалить из данных, я воспользуюсь алгоритмом SBS. Он основан на последовательном удалении признаков из полнопризнакового подмножества.Я взяла конкретно этот алгоритм, потому-что считаю что он один из немногих может обучить текстовые данные хорошо,а также хорошо может работать с данными маленького размера, так как сделан на основе логистической регрессии.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338edd4",
   "metadata": {},
   "source": [
    "*Для успешного использования алгоритма, необходимо закодировать категориальные значения.Это позволит улучшить точность выбоора признака.Также необходимо разделить выборку на тестовую и тренировочную.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e69c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl=LabelEncoder()\n",
    "# non_nomic=data.select_dtypes(exclude=[np.number])\n",
    "# non_momic_cols=non_nomic.columns.values\n",
    "# for col in non_momic_cols:\n",
    "#     data[col]=lbl.fit_transform(data[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression(penalty='l1', C=0.1)\n",
    "# lr.fit(X_train_std, y_train)\n",
    "# print('Training accuracy:', lr.score(X_train_std, y_train))\n",
    "# print('Test accuracy:', lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00101a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import clone\n",
    "# from itertools import combinations\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# if Version(sklearn_version) < '0.18':\n",
    "#     from sklearn.cross_validation import train_test_split\n",
    "# else:\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# class SBS():\n",
    "#     def __init__(self, estimator, k_features, scoring=accuracy_score,\n",
    "#                  test_size=0.25, random_state=1):\n",
    "#         self.scoring = scoring\n",
    "#         self.estimator = clone(estimator)\n",
    "#         self.k_features = k_features\n",
    "#         self.test_size = test_size\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "        \n",
    "#         X_train, X_test, y_train, y_test = \\\n",
    "#             train_test_split(X, y, test_size=self.test_size,\n",
    "#                              random_state=self.random_state)\n",
    "\n",
    "#         dim = X_train.shape[1]\n",
    "#         self.indices_ = tuple(range(dim))\n",
    "#         self.subsets_ = [self.indices_]\n",
    "#         score = self._calc_score(X_train, y_train, \n",
    "#                                  X_test, y_test, self.indices_)\n",
    "#         self.scores_ = [score]\n",
    "\n",
    "#         while dim > self.k_features:\n",
    "#             scores = []\n",
    "#             subsets = []\n",
    "\n",
    "#             for p in combinations(self.indices_, r=dim - 1):\n",
    "#                 score = self._calc_score(X_train, y_train, \n",
    "#                                          X_test, y_test, p)\n",
    "#                 scores.append(score)\n",
    "#                 subsets.append(p)\n",
    "\n",
    "#             best = np.argmax(scores)\n",
    "#             self.indices_ = subsets[best]\n",
    "#             self.subsets_.append(self.indices_)\n",
    "#             dim -= 1\n",
    "\n",
    "#             self.scores_.append(scores[best])\n",
    "#         self.k_score_ = self.scores_[-1]\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X[:, self.indices_]\n",
    "\n",
    "#     def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "#         self.estimator.fit(X_train[:, indices], y_train)\n",
    "#         y_pred = self.estimator.predict(X_test[:, indices])\n",
    "#         score = self.scoring(y_test, y_pred)\n",
    "#         return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bc7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f642a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8dbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da06be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae6a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44dcf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf398a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78c5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e19ed862",
   "metadata": {},
   "source": [
    "### 1.4 Подготовка отчета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a86750",
   "metadata": {},
   "source": [
    "*В данном модули были собраны данные с интернет-ресурса **habr.com** и с представленного файл.Произведено предварительное следование данных для того, что-бы понять, какой конкретно метод обработки использовать.Произведена предобработка данных для дальнейшего обучения модели и разработки api.Также были отобраны признаки, которые больше всего зависят от целевой переменной, а также предварительная обрабоотка текста, что бы разделение на кластеры было более качественно сделано.По итогу файл с предобработанными данными был загружен, для дальнейшего использования.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
